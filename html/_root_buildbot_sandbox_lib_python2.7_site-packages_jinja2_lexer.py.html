source file: <b>/root/buildbot/sandbox/lib/python2.7/site-packages/jinja2/lexer.py</b><br>


file stats: <b>492 lines, 152 executed: 30.9% covered</b>
<pre>
<font color="black">   1. # -*- coding: utf-8 -*-</font>
<font color="black">   2. &quot;&quot;&quot;</font>
<font color="black">   3.     jinja2.lexer</font>
<font color="black">   4.     ~~~~~~~~~~~~</font>
<font color="black">   5. </font>
<font color="black">   6.     This module implements a Jinja / Python combination lexer. The</font>
<font color="black">   7.     `Lexer` class provided by this module is used to do some preprocessing</font>
<font color="black">   8.     for Jinja.</font>
<font color="black">   9. </font>
<font color="black">  10.     On the one hand it filters out invalid operators like the bitshift</font>
<font color="black">  11.     operators we don't allow in templates. On the other hand it separates</font>
<font color="black">  12.     template code and python code in expressions.</font>
<font color="black">  13. </font>
<font color="black">  14.     :copyright: (c) 2010 by the Jinja Team.</font>
<font color="black">  15.     :license: BSD, see LICENSE for more details.</font>
<font color="green">  16. &quot;&quot;&quot;</font>
<font color="green">  17. import re</font>
<font color="black">  18. </font>
<font color="green">  19. from operator import itemgetter</font>
<font color="green">  20. from collections import deque</font>
<font color="green">  21. from jinja2.exceptions import TemplateSyntaxError</font>
<font color="green">  22. from jinja2.utils import LRUCache</font>
<font color="green">  23. from jinja2._compat import iteritems, implements_iterator, text_type, \</font>
<font color="black">  24.      intern, PY2</font>
<font color="black">  25. </font>
<font color="black">  26. </font>
<font color="black">  27. # cache for the lexers. Exists in order to be able to have multiple</font>
<font color="black">  28. # environments with the same lexer</font>
<font color="green">  29. _lexer_cache = LRUCache(50)</font>
<font color="black">  30. </font>
<font color="black">  31. # static regular expressions</font>
<font color="green">  32. whitespace_re = re.compile(r'\s+', re.U)</font>
<font color="green">  33. string_re = re.compile(r&quot;('([^'\\]*(?:\\.[^'\\]*)*)'&quot;</font>
<font color="green">  34.                        r'|&quot;([^&quot;\\]*(?:\\.[^&quot;\\]*)*)&quot;)', re.S)</font>
<font color="green">  35. integer_re = re.compile(r'\d+')</font>
<font color="black">  36. </font>
<font color="black">  37. # we use the unicode identifier rule if this python version is able</font>
<font color="black">  38. # to handle unicode identifiers, otherwise the standard ASCII one.</font>
<font color="green">  39. try:</font>
<font color="green">  40.     compile('föö', '&lt;unknown&gt;', 'eval')</font>
<font color="green">  41. except SyntaxError:</font>
<font color="green">  42.     name_re = re.compile(r'\b[a-zA-Z_][a-zA-Z0-9_]*\b')</font>
<font color="black">  43. else:</font>
<font color="red">  44.     from jinja2 import _stringdefs</font>
<font color="red">  45.     name_re = re.compile(r'[%s][%s]*' % (_stringdefs.xid_start,</font>
<font color="red">  46.                                          _stringdefs.xid_continue))</font>
<font color="black">  47. </font>
<font color="green">  48. float_re = re.compile(r'(?&lt;!\.)\d+\.\d+')</font>
<font color="green">  49. newline_re = re.compile(r'(\r\n|\r|\n)')</font>
<font color="black">  50. </font>
<font color="black">  51. # internal the tokens and keep references to them</font>
<font color="green">  52. TOKEN_ADD = intern('add')</font>
<font color="green">  53. TOKEN_ASSIGN = intern('assign')</font>
<font color="green">  54. TOKEN_COLON = intern('colon')</font>
<font color="green">  55. TOKEN_COMMA = intern('comma')</font>
<font color="green">  56. TOKEN_DIV = intern('div')</font>
<font color="green">  57. TOKEN_DOT = intern('dot')</font>
<font color="green">  58. TOKEN_EQ = intern('eq')</font>
<font color="green">  59. TOKEN_FLOORDIV = intern('floordiv')</font>
<font color="green">  60. TOKEN_GT = intern('gt')</font>
<font color="green">  61. TOKEN_GTEQ = intern('gteq')</font>
<font color="green">  62. TOKEN_LBRACE = intern('lbrace')</font>
<font color="green">  63. TOKEN_LBRACKET = intern('lbracket')</font>
<font color="green">  64. TOKEN_LPAREN = intern('lparen')</font>
<font color="green">  65. TOKEN_LT = intern('lt')</font>
<font color="green">  66. TOKEN_LTEQ = intern('lteq')</font>
<font color="green">  67. TOKEN_MOD = intern('mod')</font>
<font color="green">  68. TOKEN_MUL = intern('mul')</font>
<font color="green">  69. TOKEN_NE = intern('ne')</font>
<font color="green">  70. TOKEN_PIPE = intern('pipe')</font>
<font color="green">  71. TOKEN_POW = intern('pow')</font>
<font color="green">  72. TOKEN_RBRACE = intern('rbrace')</font>
<font color="green">  73. TOKEN_RBRACKET = intern('rbracket')</font>
<font color="green">  74. TOKEN_RPAREN = intern('rparen')</font>
<font color="green">  75. TOKEN_SEMICOLON = intern('semicolon')</font>
<font color="green">  76. TOKEN_SUB = intern('sub')</font>
<font color="green">  77. TOKEN_TILDE = intern('tilde')</font>
<font color="green">  78. TOKEN_WHITESPACE = intern('whitespace')</font>
<font color="green">  79. TOKEN_FLOAT = intern('float')</font>
<font color="green">  80. TOKEN_INTEGER = intern('integer')</font>
<font color="green">  81. TOKEN_NAME = intern('name')</font>
<font color="green">  82. TOKEN_STRING = intern('string')</font>
<font color="green">  83. TOKEN_OPERATOR = intern('operator')</font>
<font color="green">  84. TOKEN_BLOCK_BEGIN = intern('block_begin')</font>
<font color="green">  85. TOKEN_BLOCK_END = intern('block_end')</font>
<font color="green">  86. TOKEN_VARIABLE_BEGIN = intern('variable_begin')</font>
<font color="green">  87. TOKEN_VARIABLE_END = intern('variable_end')</font>
<font color="green">  88. TOKEN_RAW_BEGIN = intern('raw_begin')</font>
<font color="green">  89. TOKEN_RAW_END = intern('raw_end')</font>
<font color="green">  90. TOKEN_COMMENT_BEGIN = intern('comment_begin')</font>
<font color="green">  91. TOKEN_COMMENT_END = intern('comment_end')</font>
<font color="green">  92. TOKEN_COMMENT = intern('comment')</font>
<font color="green">  93. TOKEN_LINESTATEMENT_BEGIN = intern('linestatement_begin')</font>
<font color="green">  94. TOKEN_LINESTATEMENT_END = intern('linestatement_end')</font>
<font color="green">  95. TOKEN_LINECOMMENT_BEGIN = intern('linecomment_begin')</font>
<font color="green">  96. TOKEN_LINECOMMENT_END = intern('linecomment_end')</font>
<font color="green">  97. TOKEN_LINECOMMENT = intern('linecomment')</font>
<font color="green">  98. TOKEN_DATA = intern('data')</font>
<font color="green">  99. TOKEN_INITIAL = intern('initial')</font>
<font color="green"> 100. TOKEN_EOF = intern('eof')</font>
<font color="black"> 101. </font>
<font color="black"> 102. # bind operators to token types</font>
<font color="green"> 103. operators = {</font>
<font color="green"> 104.     '+':            TOKEN_ADD,</font>
<font color="green"> 105.     '-':            TOKEN_SUB,</font>
<font color="green"> 106.     '/':            TOKEN_DIV,</font>
<font color="green"> 107.     '//':           TOKEN_FLOORDIV,</font>
<font color="green"> 108.     '*':            TOKEN_MUL,</font>
<font color="green"> 109.     '%':            TOKEN_MOD,</font>
<font color="green"> 110.     '**':           TOKEN_POW,</font>
<font color="green"> 111.     '~':            TOKEN_TILDE,</font>
<font color="green"> 112.     '[':            TOKEN_LBRACKET,</font>
<font color="green"> 113.     ']':            TOKEN_RBRACKET,</font>
<font color="green"> 114.     '(':            TOKEN_LPAREN,</font>
<font color="green"> 115.     ')':            TOKEN_RPAREN,</font>
<font color="green"> 116.     '{':            TOKEN_LBRACE,</font>
<font color="green"> 117.     '}':            TOKEN_RBRACE,</font>
<font color="green"> 118.     '==':           TOKEN_EQ,</font>
<font color="green"> 119.     '!=':           TOKEN_NE,</font>
<font color="green"> 120.     '&gt;':            TOKEN_GT,</font>
<font color="green"> 121.     '&gt;=':           TOKEN_GTEQ,</font>
<font color="green"> 122.     '&lt;':            TOKEN_LT,</font>
<font color="green"> 123.     '&lt;=':           TOKEN_LTEQ,</font>
<font color="green"> 124.     '=':            TOKEN_ASSIGN,</font>
<font color="green"> 125.     '.':            TOKEN_DOT,</font>
<font color="green"> 126.     ':':            TOKEN_COLON,</font>
<font color="green"> 127.     '|':            TOKEN_PIPE,</font>
<font color="green"> 128.     ',':            TOKEN_COMMA,</font>
<font color="green"> 129.     ';':            TOKEN_SEMICOLON</font>
<font color="black"> 130. }</font>
<font color="black"> 131. </font>
<font color="green"> 132. reverse_operators = dict([(v, k) for k, v in iteritems(operators)])</font>
<font color="green"> 133. assert len(operators) == len(reverse_operators), 'operators dropped'</font>
<font color="green"> 134. operator_re = re.compile('(%s)' % '|'.join(re.escape(x) for x in</font>
<font color="green"> 135.                          sorted(operators, key=lambda x: -len(x))))</font>
<font color="black"> 136. </font>
<font color="green"> 137. ignored_tokens = frozenset([TOKEN_COMMENT_BEGIN, TOKEN_COMMENT,</font>
<font color="green"> 138.                             TOKEN_COMMENT_END, TOKEN_WHITESPACE,</font>
<font color="green"> 139.                             TOKEN_LINECOMMENT_BEGIN, TOKEN_LINECOMMENT_END,</font>
<font color="green"> 140.                             TOKEN_LINECOMMENT])</font>
<font color="green"> 141. ignore_if_empty = frozenset([TOKEN_WHITESPACE, TOKEN_DATA,</font>
<font color="green"> 142.                              TOKEN_COMMENT, TOKEN_LINECOMMENT])</font>
<font color="black"> 143. </font>
<font color="black"> 144. </font>
<font color="green"> 145. def _describe_token_type(token_type):</font>
<font color="red"> 146.     if token_type in reverse_operators:</font>
<font color="red"> 147.         return reverse_operators[token_type]</font>
<font color="red"> 148.     return {</font>
<font color="red"> 149.         TOKEN_COMMENT_BEGIN:        'begin of comment',</font>
<font color="red"> 150.         TOKEN_COMMENT_END:          'end of comment',</font>
<font color="red"> 151.         TOKEN_COMMENT:              'comment',</font>
<font color="red"> 152.         TOKEN_LINECOMMENT:          'comment',</font>
<font color="red"> 153.         TOKEN_BLOCK_BEGIN:          'begin of statement block',</font>
<font color="red"> 154.         TOKEN_BLOCK_END:            'end of statement block',</font>
<font color="red"> 155.         TOKEN_VARIABLE_BEGIN:       'begin of print statement',</font>
<font color="red"> 156.         TOKEN_VARIABLE_END:         'end of print statement',</font>
<font color="red"> 157.         TOKEN_LINESTATEMENT_BEGIN:  'begin of line statement',</font>
<font color="red"> 158.         TOKEN_LINESTATEMENT_END:    'end of line statement',</font>
<font color="red"> 159.         TOKEN_DATA:                 'template data / text',</font>
<font color="red"> 160.         TOKEN_EOF:                  'end of template'</font>
<font color="red"> 161.     }.get(token_type, token_type)</font>
<font color="black"> 162. </font>
<font color="black"> 163. </font>
<font color="green"> 164. def describe_token(token):</font>
<font color="black"> 165.     &quot;&quot;&quot;Returns a description of the token.&quot;&quot;&quot;</font>
<font color="red"> 166.     if token.type == 'name':</font>
<font color="red"> 167.         return token.value</font>
<font color="red"> 168.     return _describe_token_type(token.type)</font>
<font color="black"> 169. </font>
<font color="black"> 170. </font>
<font color="green"> 171. def describe_token_expr(expr):</font>
<font color="black"> 172.     &quot;&quot;&quot;Like `describe_token` but for token expressions.&quot;&quot;&quot;</font>
<font color="red"> 173.     if ':' in expr:</font>
<font color="red"> 174.         type, value = expr.split(':', 1)</font>
<font color="red"> 175.         if type == 'name':</font>
<font color="red"> 176.             return value</font>
<font color="black"> 177.     else:</font>
<font color="red"> 178.         type = expr</font>
<font color="red"> 179.     return _describe_token_type(type)</font>
<font color="black"> 180. </font>
<font color="black"> 181. </font>
<font color="green"> 182. def count_newlines(value):</font>
<font color="black"> 183.     &quot;&quot;&quot;Count the number of newline characters in the string.  This is</font>
<font color="black"> 184.     useful for extensions that filter a stream.</font>
<font color="black"> 185.     &quot;&quot;&quot;</font>
<font color="red"> 186.     return len(newline_re.findall(value))</font>
<font color="black"> 187. </font>
<font color="black"> 188. </font>
<font color="green"> 189. def compile_rules(environment):</font>
<font color="black"> 190.     &quot;&quot;&quot;Compiles all the rules from the environment into a list of rules.&quot;&quot;&quot;</font>
<font color="red"> 191.     e = re.escape</font>
<font color="black"> 192.     rules = [</font>
<font color="red"> 193.         (len(environment.comment_start_string), 'comment',</font>
<font color="red"> 194.          e(environment.comment_start_string)),</font>
<font color="red"> 195.         (len(environment.block_start_string), 'block',</font>
<font color="red"> 196.          e(environment.block_start_string)),</font>
<font color="red"> 197.         (len(environment.variable_start_string), 'variable',</font>
<font color="red"> 198.          e(environment.variable_start_string))</font>
<font color="black"> 199.     ]</font>
<font color="black"> 200. </font>
<font color="red"> 201.     if environment.line_statement_prefix is not None:</font>
<font color="red"> 202.         rules.append((len(environment.line_statement_prefix), 'linestatement',</font>
<font color="red"> 203.                       r'^[ \t\v]*' + e(environment.line_statement_prefix)))</font>
<font color="red"> 204.     if environment.line_comment_prefix is not None:</font>
<font color="red"> 205.         rules.append((len(environment.line_comment_prefix), 'linecomment',</font>
<font color="red"> 206.                       r'(?:^|(?&lt;=\S))[^\S\r\n]*' +</font>
<font color="red"> 207.                       e(environment.line_comment_prefix)))</font>
<font color="black"> 208. </font>
<font color="red"> 209.     return [x[1:] for x in sorted(rules, reverse=True)]</font>
<font color="black"> 210. </font>
<font color="black"> 211. </font>
<font color="green"> 212. class Failure(object):</font>
<font color="black"> 213.     &quot;&quot;&quot;Class that raises a `TemplateSyntaxError` if called.</font>
<font color="black"> 214.     Used by the `Lexer` to specify known errors.</font>
<font color="green"> 215.     &quot;&quot;&quot;</font>
<font color="black"> 216. </font>
<font color="green"> 217.     def __init__(self, message, cls=TemplateSyntaxError):</font>
<font color="red"> 218.         self.message = message</font>
<font color="red"> 219.         self.error_class = cls</font>
<font color="black"> 220. </font>
<font color="green"> 221.     def __call__(self, lineno, filename):</font>
<font color="red"> 222.         raise self.error_class(self.message, lineno, filename)</font>
<font color="black"> 223. </font>
<font color="black"> 224. </font>
<font color="green"> 225. class Token(tuple):</font>
<font color="green"> 226.     &quot;&quot;&quot;Token class.&quot;&quot;&quot;</font>
<font color="green"> 227.     __slots__ = ()</font>
<font color="green"> 228.     lineno, type, value = (property(itemgetter(x)) for x in range(3))</font>
<font color="black"> 229. </font>
<font color="green"> 230.     def __new__(cls, lineno, type, value):</font>
<font color="red"> 231.         return tuple.__new__(cls, (lineno, intern(str(type)), value))</font>
<font color="black"> 232. </font>
<font color="green"> 233.     def __str__(self):</font>
<font color="red"> 234.         if self.type in reverse_operators:</font>
<font color="red"> 235.             return reverse_operators[self.type]</font>
<font color="red"> 236.         elif self.type == 'name':</font>
<font color="red"> 237.             return self.value</font>
<font color="red"> 238.         return self.type</font>
<font color="black"> 239. </font>
<font color="green"> 240.     def test(self, expr):</font>
<font color="black"> 241.         &quot;&quot;&quot;Test a token against a token expression.  This can either be a</font>
<font color="black"> 242.         token type or ``'token_type:token_value'``.  This can only test</font>
<font color="black"> 243.         against string values and types.</font>
<font color="black"> 244.         &quot;&quot;&quot;</font>
<font color="black"> 245.         # here we do a regular string equality check as test_any is usually</font>
<font color="black"> 246.         # passed an iterable of not interned strings.</font>
<font color="red"> 247.         if self.type == expr:</font>
<font color="red"> 248.             return True</font>
<font color="red"> 249.         elif ':' in expr:</font>
<font color="red"> 250.             return expr.split(':', 1) == [self.type, self.value]</font>
<font color="red"> 251.         return False</font>
<font color="black"> 252. </font>
<font color="green"> 253.     def test_any(self, *iterable):</font>
<font color="black"> 254.         &quot;&quot;&quot;Test against multiple token expressions.&quot;&quot;&quot;</font>
<font color="red"> 255.         for expr in iterable:</font>
<font color="red"> 256.             if self.test(expr):</font>
<font color="red"> 257.                 return True</font>
<font color="red"> 258.         return False</font>
<font color="black"> 259. </font>
<font color="green"> 260.     def __repr__(self):</font>
<font color="red"> 261.         return 'Token(%r, %r, %r)' % (</font>
<font color="red"> 262.             self.lineno,</font>
<font color="red"> 263.             self.type,</font>
<font color="red"> 264.             self.value</font>
<font color="black"> 265.         )</font>
<font color="black"> 266. </font>
<font color="black"> 267. </font>
<font color="green"> 268. @implements_iterator</font>
<font color="green"> 269. class TokenStreamIterator(object):</font>
<font color="black"> 270.     &quot;&quot;&quot;The iterator for tokenstreams.  Iterate over the stream</font>
<font color="black"> 271.     until the eof token is reached.</font>
<font color="green"> 272.     &quot;&quot;&quot;</font>
<font color="black"> 273. </font>
<font color="green"> 274.     def __init__(self, stream):</font>
<font color="red"> 275.         self.stream = stream</font>
<font color="black"> 276. </font>
<font color="green"> 277.     def __iter__(self):</font>
<font color="red"> 278.         return self</font>
<font color="black"> 279. </font>
<font color="green"> 280.     def __next__(self):</font>
<font color="red"> 281.         token = self.stream.current</font>
<font color="red"> 282.         if token.type is TOKEN_EOF:</font>
<font color="red"> 283.             self.stream.close()</font>
<font color="red"> 284.             raise StopIteration()</font>
<font color="red"> 285.         next(self.stream)</font>
<font color="red"> 286.         return token</font>
<font color="black"> 287. </font>
<font color="black"> 288. </font>
<font color="green"> 289. @implements_iterator</font>
<font color="green"> 290. class TokenStream(object):</font>
<font color="black"> 291.     &quot;&quot;&quot;A token stream is an iterable that yields :class:`Token`\s.  The</font>
<font color="black"> 292.     parser however does not iterate over it but calls :meth:`next` to go</font>
<font color="black"> 293.     one token ahead.  The current active token is stored as :attr:`current`.</font>
<font color="green"> 294.     &quot;&quot;&quot;</font>
<font color="black"> 295. </font>
<font color="green"> 296.     def __init__(self, generator, name, filename):</font>
<font color="red"> 297.         self._iter = iter(generator)</font>
<font color="red"> 298.         self._pushed = deque()</font>
<font color="red"> 299.         self.name = name</font>
<font color="red"> 300.         self.filename = filename</font>
<font color="red"> 301.         self.closed = False</font>
<font color="red"> 302.         self.current = Token(1, TOKEN_INITIAL, '')</font>
<font color="red"> 303.         next(self)</font>
<font color="black"> 304. </font>
<font color="green"> 305.     def __iter__(self):</font>
<font color="red"> 306.         return TokenStreamIterator(self)</font>
<font color="black"> 307. </font>
<font color="green"> 308.     def __bool__(self):</font>
<font color="red"> 309.         return bool(self._pushed) or self.current.type is not TOKEN_EOF</font>
<font color="green"> 310.     __nonzero__ = __bool__  # py2</font>
<font color="black"> 311. </font>
<font color="green"> 312.     eos = property(lambda x: not x, doc=&quot;Are we at the end of the stream?&quot;)</font>
<font color="black"> 313. </font>
<font color="green"> 314.     def push(self, token):</font>
<font color="black"> 315.         &quot;&quot;&quot;Push a token back to the stream.&quot;&quot;&quot;</font>
<font color="red"> 316.         self._pushed.append(token)</font>
<font color="black"> 317. </font>
<font color="green"> 318.     def look(self):</font>
<font color="black"> 319.         &quot;&quot;&quot;Look at the next token.&quot;&quot;&quot;</font>
<font color="red"> 320.         old_token = next(self)</font>
<font color="red"> 321.         result = self.current</font>
<font color="red"> 322.         self.push(result)</font>
<font color="red"> 323.         self.current = old_token</font>
<font color="red"> 324.         return result</font>
<font color="black"> 325. </font>
<font color="green"> 326.     def skip(self, n=1):</font>
<font color="black"> 327.         &quot;&quot;&quot;Got n tokens ahead.&quot;&quot;&quot;</font>
<font color="red"> 328.         for x in range(n):</font>
<font color="red"> 329.             next(self)</font>
<font color="black"> 330. </font>
<font color="green"> 331.     def next_if(self, expr):</font>
<font color="black"> 332.         &quot;&quot;&quot;Perform the token test and return the token if it matched.</font>
<font color="black"> 333.         Otherwise the return value is `None`.</font>
<font color="black"> 334.         &quot;&quot;&quot;</font>
<font color="red"> 335.         if self.current.test(expr):</font>
<font color="red"> 336.             return next(self)</font>
<font color="black"> 337. </font>
<font color="green"> 338.     def skip_if(self, expr):</font>
<font color="black"> 339.         &quot;&quot;&quot;Like :meth:`next_if` but only returns `True` or `False`.&quot;&quot;&quot;</font>
<font color="red"> 340.         return self.next_if(expr) is not None</font>
<font color="black"> 341. </font>
<font color="green"> 342.     def __next__(self):</font>
<font color="black"> 343.         &quot;&quot;&quot;Go one token ahead and return the old one&quot;&quot;&quot;</font>
<font color="red"> 344.         rv = self.current</font>
<font color="red"> 345.         if self._pushed:</font>
<font color="red"> 346.             self.current = self._pushed.popleft()</font>
<font color="red"> 347.         elif self.current.type is not TOKEN_EOF:</font>
<font color="red"> 348.             try:</font>
<font color="red"> 349.                 self.current = next(self._iter)</font>
<font color="red"> 350.             except StopIteration:</font>
<font color="red"> 351.                 self.close()</font>
<font color="red"> 352.         return rv</font>
<font color="black"> 353. </font>
<font color="green"> 354.     def close(self):</font>
<font color="black"> 355.         &quot;&quot;&quot;Close the stream.&quot;&quot;&quot;</font>
<font color="red"> 356.         self.current = Token(self.current.lineno, TOKEN_EOF, '')</font>
<font color="red"> 357.         self._iter = None</font>
<font color="red"> 358.         self.closed = True</font>
<font color="black"> 359. </font>
<font color="green"> 360.     def expect(self, expr):</font>
<font color="black"> 361.         &quot;&quot;&quot;Expect a given token type and return it.  This accepts the same</font>
<font color="black"> 362.         argument as :meth:`jinja2.lexer.Token.test`.</font>
<font color="black"> 363.         &quot;&quot;&quot;</font>
<font color="red"> 364.         if not self.current.test(expr):</font>
<font color="red"> 365.             expr = describe_token_expr(expr)</font>
<font color="red"> 366.             if self.current.type is TOKEN_EOF:</font>
<font color="red"> 367.                 raise TemplateSyntaxError('unexpected end of template, '</font>
<font color="red"> 368.                                           'expected %r.' % expr,</font>
<font color="red"> 369.                                           self.current.lineno,</font>
<font color="red"> 370.                                           self.name, self.filename)</font>
<font color="red"> 371.             raise TemplateSyntaxError(&quot;expected token %r, got %r&quot; %</font>
<font color="red"> 372.                                       (expr, describe_token(self.current)),</font>
<font color="red"> 373.                                       self.current.lineno,</font>
<font color="red"> 374.                                       self.name, self.filename)</font>
<font color="red"> 375.         try:</font>
<font color="red"> 376.             return self.current</font>
<font color="black"> 377.         finally:</font>
<font color="red"> 378.             next(self)</font>
<font color="black"> 379. </font>
<font color="black"> 380. </font>
<font color="green"> 381. def get_lexer(environment):</font>
<font color="black"> 382.     &quot;&quot;&quot;Return a lexer which is probably cached.&quot;&quot;&quot;</font>
<font color="red"> 383.     key = (environment.block_start_string,</font>
<font color="red"> 384.            environment.block_end_string,</font>
<font color="red"> 385.            environment.variable_start_string,</font>
<font color="red"> 386.            environment.variable_end_string,</font>
<font color="red"> 387.            environment.comment_start_string,</font>
<font color="red"> 388.            environment.comment_end_string,</font>
<font color="red"> 389.            environment.line_statement_prefix,</font>
<font color="red"> 390.            environment.line_comment_prefix,</font>
<font color="red"> 391.            environment.trim_blocks,</font>
<font color="red"> 392.            environment.lstrip_blocks,</font>
<font color="red"> 393.            environment.newline_sequence,</font>
<font color="red"> 394.            environment.keep_trailing_newline)</font>
<font color="red"> 395.     lexer = _lexer_cache.get(key)</font>
<font color="red"> 396.     if lexer is None:</font>
<font color="red"> 397.         lexer = Lexer(environment)</font>
<font color="red"> 398.         _lexer_cache[key] = lexer</font>
<font color="red"> 399.     return lexer</font>
<font color="black"> 400. </font>
<font color="black"> 401. </font>
<font color="green"> 402. class Lexer(object):</font>
<font color="black"> 403.     &quot;&quot;&quot;Class that implements a lexer for a given environment. Automatically</font>
<font color="black"> 404.     created by the environment class, usually you don't have to do that.</font>
<font color="black"> 405. </font>
<font color="black"> 406.     Note that the lexer is not automatically bound to an environment.</font>
<font color="black"> 407.     Multiple environments can share the same lexer.</font>
<font color="green"> 408.     &quot;&quot;&quot;</font>
<font color="black"> 409. </font>
<font color="green"> 410.     def __init__(self, environment):</font>
<font color="black"> 411.         # shortcuts</font>
<font color="red"> 412.         c = lambda x: re.compile(x, re.M | re.S)</font>
<font color="red"> 413.         e = re.escape</font>
<font color="black"> 414. </font>
<font color="black"> 415.         # lexing rules for tags</font>
<font color="black"> 416.         tag_rules = [</font>
<font color="red"> 417.             (whitespace_re, TOKEN_WHITESPACE, None),</font>
<font color="red"> 418.             (float_re, TOKEN_FLOAT, None),</font>
<font color="red"> 419.             (integer_re, TOKEN_INTEGER, None),</font>
<font color="red"> 420.             (name_re, TOKEN_NAME, None),</font>
<font color="red"> 421.             (string_re, TOKEN_STRING, None),</font>
<font color="red"> 422.             (operator_re, TOKEN_OPERATOR, None)</font>
<font color="black"> 423.         ]</font>
<font color="black"> 424. </font>
<font color="black"> 425.         # assemble the root lexing rule. because &quot;|&quot; is ungreedy</font>
<font color="black"> 426.         # we have to sort by length so that the lexer continues working</font>
<font color="black"> 427.         # as expected when we have parsing rules like &lt;% for block and</font>
<font color="black"> 428.         # &lt;%= for variables. (if someone wants asp like syntax)</font>
<font color="black"> 429.         # variables are just part of the rules if variable processing</font>
<font color="black"> 430.         # is required.</font>
<font color="red"> 431.         root_tag_rules = compile_rules(environment)</font>
<font color="black"> 432. </font>
<font color="black"> 433.         # block suffix if trimming is enabled</font>
<font color="red"> 434.         block_suffix_re = environment.trim_blocks and '\\n?' or ''</font>
<font color="black"> 435. </font>
<font color="black"> 436.         # strip leading spaces if lstrip_blocks is enabled</font>
<font color="red"> 437.         prefix_re = {}</font>
<font color="red"> 438.         if environment.lstrip_blocks:</font>
<font color="black"> 439.             # use '{%+' to manually disable lstrip_blocks behavior</font>
<font color="red"> 440.             no_lstrip_re = e('+')</font>
<font color="black"> 441.             # detect overlap between block and variable or comment strings</font>
<font color="red"> 442.             block_diff = c(r'^%s(.*)' % e(environment.block_start_string))</font>
<font color="black"> 443.             # make sure we don't mistake a block for a variable or a comment</font>
<font color="red"> 444.             m = block_diff.match(environment.comment_start_string)</font>
<font color="red"> 445.             no_lstrip_re += m and r'|%s' % e(m.group(1)) or ''</font>
<font color="red"> 446.             m = block_diff.match(environment.variable_start_string)</font>
<font color="red"> 447.             no_lstrip_re += m and r'|%s' % e(m.group(1)) or ''</font>
<font color="black"> 448. </font>
<font color="black"> 449.             # detect overlap between comment and variable strings</font>
<font color="red"> 450.             comment_diff = c(r'^%s(.*)' % e(environment.comment_start_string))</font>
<font color="red"> 451.             m = comment_diff.match(environment.variable_start_string)</font>
<font color="red"> 452.             no_variable_re = m and r'(?!%s)' % e(m.group(1)) or ''</font>
<font color="black"> 453. </font>
<font color="red"> 454.             lstrip_re = r'^[ \t]*'</font>
<font color="red"> 455.             block_prefix_re = r'%s%s(?!%s)|%s\+?' % (</font>
<font color="red"> 456.                     lstrip_re,</font>
<font color="red"> 457.                     e(environment.block_start_string),</font>
<font color="red"> 458.                     no_lstrip_re,</font>
<font color="red"> 459.                     e(environment.block_start_string),</font>
<font color="black"> 460.                     )</font>
<font color="red"> 461.             comment_prefix_re = r'%s%s%s|%s\+?' % (</font>
<font color="red"> 462.                     lstrip_re,</font>
<font color="red"> 463.                     e(environment.comment_start_string),</font>
<font color="red"> 464.                     no_variable_re,</font>
<font color="red"> 465.                     e(environment.comment_start_string),</font>
<font color="black"> 466.                     )</font>
<font color="red"> 467.             prefix_re['block'] = block_prefix_re</font>
<font color="red"> 468.             prefix_re['comment'] = comment_prefix_re</font>
<font color="black"> 469.         else:</font>
<font color="red"> 470.             block_prefix_re = '%s' % e(environment.block_start_string)</font>
<font color="black"> 471. </font>
<font color="red"> 472.         self.newline_sequence = environment.newline_sequence</font>
<font color="red"> 473.         self.keep_trailing_newline = environment.keep_trailing_newline</font>
<font color="black"> 474. </font>
<font color="black"> 475.         # global lexing rules</font>
<font color="red"> 476.         self.rules = {</font>
<font color="black"> 477.             'root': [</font>
<font color="black"> 478.                 # directives</font>
<font color="red"> 479.                 (c('(.*?)(?:%s)' % '|'.join(</font>
<font color="red"> 480.                     [r'(?P&lt;raw_begin&gt;(?:\s*%s\-|%s)\s*raw\s*(?:\-%s\s*|%s))' % (</font>
<font color="red"> 481.                         e(environment.block_start_string),</font>
<font color="red"> 482.                         block_prefix_re,</font>
<font color="red"> 483.                         e(environment.block_end_string),</font>
<font color="red"> 484.                         e(environment.block_end_string)</font>
<font color="black"> 485.                     )] + [</font>
<font color="red"> 486.                         r'(?P&lt;%s_begin&gt;\s*%s\-|%s)' % (n, r, prefix_re.get(n,r))</font>
<font color="red"> 487.                         for n, r in root_tag_rules</font>
<font color="red"> 488.                     ])), (TOKEN_DATA, '#bygroup'), '#bygroup'),</font>
<font color="black"> 489.                 # data</font>
<font color="red"> 490.                 (c('.+'), TOKEN_DATA, None)</font>
<font color="black"> 491.             ],</font>
<font color="black"> 492.             # comments</font>
<font color="black"> 493.             TOKEN_COMMENT_BEGIN: [</font>
<font color="red"> 494.                 (c(r'(.*?)((?:\-%s\s*|%s)%s)' % (</font>
<font color="red"> 495.                     e(environment.comment_end_string),</font>
<font color="red"> 496.                     e(environment.comment_end_string),</font>
<font color="red"> 497.                     block_suffix_re</font>
<font color="red"> 498.                 )), (TOKEN_COMMENT, TOKEN_COMMENT_END), '#pop'),</font>
<font color="red"> 499.                 (c('(.)'), (Failure('Missing end of comment tag'),), None)</font>
<font color="black"> 500.             ],</font>
<font color="black"> 501.             # blocks</font>
<font color="black"> 502.             TOKEN_BLOCK_BEGIN: [</font>
<font color="red"> 503.                 (c('(?:\-%s\s*|%s)%s' % (</font>
<font color="red"> 504.                     e(environment.block_end_string),</font>
<font color="red"> 505.                     e(environment.block_end_string),</font>
<font color="red"> 506.                     block_suffix_re</font>
<font color="red"> 507.                 )), TOKEN_BLOCK_END, '#pop'),</font>
<font color="red"> 508.             ] + tag_rules,</font>
<font color="black"> 509.             # variables</font>
<font color="black"> 510.             TOKEN_VARIABLE_BEGIN: [</font>
<font color="red"> 511.                 (c('\-%s\s*|%s' % (</font>
<font color="red"> 512.                     e(environment.variable_end_string),</font>
<font color="red"> 513.                     e(environment.variable_end_string)</font>
<font color="red"> 514.                 )), TOKEN_VARIABLE_END, '#pop')</font>
<font color="red"> 515.             ] + tag_rules,</font>
<font color="black"> 516.             # raw block</font>
<font color="black"> 517.             TOKEN_RAW_BEGIN: [</font>
<font color="red"> 518.                 (c('(.*?)((?:\s*%s\-|%s)\s*endraw\s*(?:\-%s\s*|%s%s))' % (</font>
<font color="red"> 519.                     e(environment.block_start_string),</font>
<font color="red"> 520.                     block_prefix_re,</font>
<font color="red"> 521.                     e(environment.block_end_string),</font>
<font color="red"> 522.                     e(environment.block_end_string),</font>
<font color="red"> 523.                     block_suffix_re</font>
<font color="red"> 524.                 )), (TOKEN_DATA, TOKEN_RAW_END), '#pop'),</font>
<font color="red"> 525.                 (c('(.)'), (Failure('Missing end of raw directive'),), None)</font>
<font color="black"> 526.             ],</font>
<font color="black"> 527.             # line statements</font>
<font color="black"> 528.             TOKEN_LINESTATEMENT_BEGIN: [</font>
<font color="red"> 529.                 (c(r'\s*(\n|$)'), TOKEN_LINESTATEMENT_END, '#pop')</font>
<font color="red"> 530.             ] + tag_rules,</font>
<font color="black"> 531.             # line comments</font>
<font color="black"> 532.             TOKEN_LINECOMMENT_BEGIN: [</font>
<font color="red"> 533.                 (c(r'(.*?)()(?=\n|$)'), (TOKEN_LINECOMMENT,</font>
<font color="red"> 534.                  TOKEN_LINECOMMENT_END), '#pop')</font>
<font color="black"> 535.             ]</font>
<font color="black"> 536.         }</font>
<font color="black"> 537. </font>
<font color="green"> 538.     def _normalize_newlines(self, value):</font>
<font color="black"> 539.         &quot;&quot;&quot;Called for strings and template data to normalize it to unicode.&quot;&quot;&quot;</font>
<font color="red"> 540.         return newline_re.sub(self.newline_sequence, value)</font>
<font color="black"> 541. </font>
<font color="green"> 542.     def tokenize(self, source, name=None, filename=None, state=None):</font>
<font color="black"> 543.         &quot;&quot;&quot;Calls tokeniter + tokenize and wraps it in a token stream.</font>
<font color="black"> 544.         &quot;&quot;&quot;</font>
<font color="red"> 545.         stream = self.tokeniter(source, name, filename, state)</font>
<font color="red"> 546.         return TokenStream(self.wrap(stream, name, filename), name, filename)</font>
<font color="black"> 547. </font>
<font color="green"> 548.     def wrap(self, stream, name=None, filename=None):</font>
<font color="black"> 549.         &quot;&quot;&quot;This is called with the stream as returned by `tokenize` and wraps</font>
<font color="black"> 550.         every token in a :class:`Token` and converts the value.</font>
<font color="black"> 551.         &quot;&quot;&quot;</font>
<font color="red"> 552.         for lineno, token, value in stream:</font>
<font color="red"> 553.             if token in ignored_tokens:</font>
<font color="red"> 554.                 continue</font>
<font color="red"> 555.             elif token == 'linestatement_begin':</font>
<font color="red"> 556.                 token = 'block_begin'</font>
<font color="red"> 557.             elif token == 'linestatement_end':</font>
<font color="red"> 558.                 token = 'block_end'</font>
<font color="black"> 559.             # we are not interested in those tokens in the parser</font>
<font color="red"> 560.             elif token in ('raw_begin', 'raw_end'):</font>
<font color="red"> 561.                 continue</font>
<font color="red"> 562.             elif token == 'data':</font>
<font color="red"> 563.                 value = self._normalize_newlines(value)</font>
<font color="red"> 564.             elif token == 'keyword':</font>
<font color="red"> 565.                 token = value</font>
<font color="red"> 566.             elif token == 'name':</font>
<font color="red"> 567.                 value = str(value)</font>
<font color="red"> 568.             elif token == 'string':</font>
<font color="black"> 569.                 # try to unescape string</font>
<font color="red"> 570.                 try:</font>
<font color="red"> 571.                     value = self._normalize_newlines(value[1:-1]) \</font>
<font color="red"> 572.                         .encode('ascii', 'backslashreplace') \</font>
<font color="red"> 573.                         .decode('unicode-escape')</font>
<font color="red"> 574.                 except Exception as e:</font>
<font color="red"> 575.                     msg = str(e).split(':')[-1].strip()</font>
<font color="red"> 576.                     raise TemplateSyntaxError(msg, lineno, name, filename)</font>
<font color="black"> 577.                 # if we can express it as bytestring (ascii only)</font>
<font color="black"> 578.                 # we do that for support of semi broken APIs</font>
<font color="black"> 579.                 # as datetime.datetime.strftime.  On python 3 this</font>
<font color="black"> 580.                 # call becomes a noop thanks to 2to3</font>
<font color="red"> 581.                 if PY2:</font>
<font color="red"> 582.                     try:</font>
<font color="red"> 583.                         value = value.encode('ascii')</font>
<font color="red"> 584.                     except UnicodeError:</font>
<font color="red"> 585.                         pass</font>
<font color="red"> 586.             elif token == 'integer':</font>
<font color="red"> 587.                 value = int(value)</font>
<font color="red"> 588.             elif token == 'float':</font>
<font color="red"> 589.                 value = float(value)</font>
<font color="red"> 590.             elif token == 'operator':</font>
<font color="red"> 591.                 token = operators[value]</font>
<font color="red"> 592.             yield Token(lineno, token, value)</font>
<font color="black"> 593. </font>
<font color="green"> 594.     def tokeniter(self, source, name, filename=None, state=None):</font>
<font color="black"> 595.         &quot;&quot;&quot;This method tokenizes the text and returns the tokens in a</font>
<font color="black"> 596.         generator.  Use this method if you just want to tokenize a template.</font>
<font color="black"> 597.         &quot;&quot;&quot;</font>
<font color="red"> 598.         source = text_type(source)</font>
<font color="red"> 599.         lines = source.splitlines()</font>
<font color="red"> 600.         if self.keep_trailing_newline and source:</font>
<font color="red"> 601.             for newline in ('\r\n', '\r', '\n'):</font>
<font color="red"> 602.                 if source.endswith(newline):</font>
<font color="red"> 603.                     lines.append('')</font>
<font color="red"> 604.                     break</font>
<font color="red"> 605.         source = '\n'.join(lines)</font>
<font color="red"> 606.         pos = 0</font>
<font color="red"> 607.         lineno = 1</font>
<font color="red"> 608.         stack = ['root']</font>
<font color="red"> 609.         if state is not None and state != 'root':</font>
<font color="red"> 610.             assert state in ('variable', 'block'), 'invalid state'</font>
<font color="red"> 611.             stack.append(state + '_begin')</font>
<font color="black"> 612.         else:</font>
<font color="red"> 613.             state = 'root'</font>
<font color="red"> 614.         statetokens = self.rules[stack[-1]]</font>
<font color="red"> 615.         source_length = len(source)</font>
<font color="black"> 616. </font>
<font color="red"> 617.         balancing_stack = []</font>
<font color="black"> 618. </font>
<font color="red"> 619.         while 1:</font>
<font color="black"> 620.             # tokenizer loop</font>
<font color="red"> 621.             for regex, tokens, new_state in statetokens:</font>
<font color="red"> 622.                 m = regex.match(source, pos)</font>
<font color="black"> 623.                 # if no match we try again with the next rule</font>
<font color="red"> 624.                 if m is None:</font>
<font color="red"> 625.                     continue</font>
<font color="black"> 626. </font>
<font color="black"> 627.                 # we only match blocks and variables if braces / parentheses</font>
<font color="black"> 628.                 # are balanced. continue parsing with the lower rule which</font>
<font color="black"> 629.                 # is the operator rule. do this only if the end tags look</font>
<font color="black"> 630.                 # like operators</font>
<font color="red"> 631.                 if balancing_stack and \</font>
<font color="red"> 632.                    tokens in ('variable_end', 'block_end',</font>
<font color="red"> 633.                               'linestatement_end'):</font>
<font color="red"> 634.                     continue</font>
<font color="black"> 635. </font>
<font color="black"> 636.                 # tuples support more options</font>
<font color="red"> 637.                 if isinstance(tokens, tuple):</font>
<font color="red"> 638.                     for idx, token in enumerate(tokens):</font>
<font color="black"> 639.                         # failure group</font>
<font color="red"> 640.                         if token.__class__ is Failure:</font>
<font color="red"> 641.                             raise token(lineno, filename)</font>
<font color="black"> 642.                         # bygroup is a bit more complex, in that case we</font>
<font color="black"> 643.                         # yield for the current token the first named</font>
<font color="black"> 644.                         # group that matched</font>
<font color="red"> 645.                         elif token == '#bygroup':</font>
<font color="red"> 646.                             for key, value in iteritems(m.groupdict()):</font>
<font color="red"> 647.                                 if value is not None:</font>
<font color="red"> 648.                                     yield lineno, key, value</font>
<font color="red"> 649.                                     lineno += value.count('\n')</font>
<font color="red"> 650.                                     break</font>
<font color="black"> 651.                             else:</font>
<font color="red"> 652.                                 raise RuntimeError('%r wanted to resolve '</font>
<font color="black"> 653.                                                    'the token dynamically'</font>
<font color="black"> 654.                                                    ' but no group matched'</font>
<font color="red"> 655.                                                    % regex)</font>
<font color="black"> 656.                         # normal group</font>
<font color="black"> 657.                         else:</font>
<font color="red"> 658.                             data = m.group(idx + 1)</font>
<font color="red"> 659.                             if data or token not in ignore_if_empty:</font>
<font color="red"> 660.                                 yield lineno, token, data</font>
<font color="red"> 661.                             lineno += data.count('\n')</font>
<font color="black"> 662. </font>
<font color="black"> 663.                 # strings as token just are yielded as it.</font>
<font color="black"> 664.                 else:</font>
<font color="red"> 665.                     data = m.group()</font>
<font color="black"> 666.                     # update brace/parentheses balance</font>
<font color="red"> 667.                     if tokens == 'operator':</font>
<font color="red"> 668.                         if data == '{':</font>
<font color="red"> 669.                             balancing_stack.append('}')</font>
<font color="red"> 670.                         elif data == '(':</font>
<font color="red"> 671.                             balancing_stack.append(')')</font>
<font color="red"> 672.                         elif data == '[':</font>
<font color="red"> 673.                             balancing_stack.append(']')</font>
<font color="red"> 674.                         elif data in ('}', ')', ']'):</font>
<font color="red"> 675.                             if not balancing_stack:</font>
<font color="red"> 676.                                 raise TemplateSyntaxError('unexpected \'%s\'' %</font>
<font color="red"> 677.                                                           data, lineno, name,</font>
<font color="red"> 678.                                                           filename)</font>
<font color="red"> 679.                             expected_op = balancing_stack.pop()</font>
<font color="red"> 680.                             if expected_op != data:</font>
<font color="red"> 681.                                 raise TemplateSyntaxError('unexpected \'%s\', '</font>
<font color="black"> 682.                                                           'expected \'%s\'' %</font>
<font color="red"> 683.                                                           (data, expected_op),</font>
<font color="red"> 684.                                                           lineno, name,</font>
<font color="red"> 685.                                                           filename)</font>
<font color="black"> 686.                     # yield items</font>
<font color="red"> 687.                     if data or tokens not in ignore_if_empty:</font>
<font color="red"> 688.                         yield lineno, tokens, data</font>
<font color="red"> 689.                     lineno += data.count('\n')</font>
<font color="black"> 690. </font>
<font color="black"> 691.                 # fetch new position into new variable so that we can check</font>
<font color="black"> 692.                 # if there is a internal parsing error which would result</font>
<font color="black"> 693.                 # in an infinite loop</font>
<font color="red"> 694.                 pos2 = m.end()</font>
<font color="black"> 695. </font>
<font color="black"> 696.                 # handle state changes</font>
<font color="red"> 697.                 if new_state is not None:</font>
<font color="black"> 698.                     # remove the uppermost state</font>
<font color="red"> 699.                     if new_state == '#pop':</font>
<font color="red"> 700.                         stack.pop()</font>
<font color="black"> 701.                     # resolve the new state by group checking</font>
<font color="red"> 702.                     elif new_state == '#bygroup':</font>
<font color="red"> 703.                         for key, value in iteritems(m.groupdict()):</font>
<font color="red"> 704.                             if value is not None:</font>
<font color="red"> 705.                                 stack.append(key)</font>
<font color="red"> 706.                                 break</font>
<font color="black"> 707.                         else:</font>
<font color="red"> 708.                             raise RuntimeError('%r wanted to resolve the '</font>
<font color="black"> 709.                                                'new state dynamically but'</font>
<font color="black"> 710.                                                ' no group matched' %</font>
<font color="red"> 711.                                                regex)</font>
<font color="black"> 712.                     # direct state name given</font>
<font color="black"> 713.                     else:</font>
<font color="red"> 714.                         stack.append(new_state)</font>
<font color="red"> 715.                     statetokens = self.rules[stack[-1]]</font>
<font color="black"> 716.                 # we are still at the same position and no stack change.</font>
<font color="black"> 717.                 # this means a loop without break condition, avoid that and</font>
<font color="black"> 718.                 # raise error</font>
<font color="red"> 719.                 elif pos2 == pos:</font>
<font color="red"> 720.                     raise RuntimeError('%r yielded empty string without '</font>
<font color="red"> 721.                                        'stack change' % regex)</font>
<font color="black"> 722.                 # publish new function and start again</font>
<font color="red"> 723.                 pos = pos2</font>
<font color="red"> 724.                 break</font>
<font color="black"> 725.             # if loop terminated without break we haven't found a single match</font>
<font color="black"> 726.             # either we are at the end of the file or we have a problem</font>
<font color="black"> 727.             else:</font>
<font color="black"> 728.                 # end of text</font>
<font color="red"> 729.                 if pos &gt;= source_length:</font>
<font color="red"> 730.                     return</font>
<font color="black"> 731.                 # something went wrong</font>
<font color="red"> 732.                 raise TemplateSyntaxError('unexpected char %r at %d' %</font>
<font color="red"> 733.                                           (source[pos], pos), lineno,</font>
<font color="red"> 734.                                           name, filename)</font>
</pre>

